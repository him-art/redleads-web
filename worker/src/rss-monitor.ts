
import dotenv from 'dotenv';
import path from 'path';
import { createClient } from '@supabase/supabase-js';
import Parser from 'rss-parser';

// Load environment variables
dotenv.config({ path: path.resolve(__dirname, '../../.env.local') });

const SUPABASE_URL = process.env.NEXT_PUBLIC_SUPABASE_URL!;
const SUPABASE_SERVICE_ROLE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY!;

import { groq } from '../../lib/groq';

const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY);

// --- Heartbeat & Settings Helpers ---
async function sendHeartbeat() {
    try {
        const potentialKeys = [
            process.env.GROQ_API_KEY,
            process.env.GROQ_API_KEY_2,
            process.env.GROQ_API_KEY_3,
            process.env.GROQ_API_KEY_4,
            process.env.GROQ_API_KEY_5,
            process.env.GROQ_API_KEY_6
        ];
        const keyCount = potentialKeys.filter(k => !!k).length;

        await supabase.from('worker_status').upsert({
            id: 'sentinel',
            last_heartbeat: new Date().toISOString(),
            status: 'active',
            active_keys_count: keyCount
        });
    } catch (e) {
        console.error('üíì Heartbeat failed:', e);
    }
}

async function isApprovalRequired(): Promise<boolean> {
    try {
        const { data } = await supabase
            .from('system_settings')
            .select('value')
            .eq('key', 'report_approval_required')
            .single();
        return data?.value === true || data?.value === 'true';
    } catch (e) {
        return false;
    }
}

// Report Generation Helper
async function generateDailyReport() {
    console.log('üìù Running Scheduled Daily Report...');
    const yesterday = new Date();
    yesterday.setDate(yesterday.getDate() - 1); // Last 24h

    // 1. Get all active profiles
    const { data: profiles } = await supabase.from('profiles').select('id, email, description').not('email', 'is', null);
    if (!profiles) return;

    for (const user of profiles) {
        // 2. Fetch leads in last 24h
        const { data: newLeads } = await supabase
            .from('monitored_leads')
            .select('*')
            .eq('user_id', user.id)
            .gte('created_at', yesterday.toISOString());

        if (newLeads && newLeads.length > 0) {
            // 3. Generate High Quality Summary
            const aiSummary = await generateAISummary(newLeads, user.description || "");

            // 4. Check dynamic approval setting
            const approvalRequired = await isApprovalRequired();
            const status = approvalRequired ? 'draft' : 'ready';

            // 5. Generate HTML
            const reportHtml = `
                <div style="font-family: sans-serif; padding: 20px; background: #000; color: #fff;">
                    <h2 style="color: #F97316;">Daily Intel Report</h2>
                    
                    ${aiSummary ? `
                    <div style="margin-bottom: 24px; padding: 15px; border-left: 4px solid #F97316; background: #111;">
                        <p style="margin: 0; font-style: italic; color: #ccc; line-height: 1.5;">"${aiSummary}"</p>
                    </div>
                    ` : ''}

                    <p style="font-size: 14px; color: #888;">Matches from the last 24h:</p>
                    ${newLeads.map(l => `
                        <div style="margin: 10px 0; padding: 12px; border: 1px solid #333; border-radius: 8px;">
                            <div style="color: #F97316; font-size: 11px; font-weight: bold; margin-bottom: 4px;">r/${l.subreddit}</div>
                            <a href="${l.url}" style="color: #fff; font-weight: bold; text-decoration: none; font-size: 15px;">${l.title}</a>
                        </div>
                    `).join('')}
                </div>
            `;

            // 5. Save Draft
            await supabase.from('email_drafts').insert({
                user_id: user.id,
                subject: `Daily Intel: ${newLeads.length} New Opportunities`,
                body_html: reportHtml,
                status: status
            });
            console.log(`‚úÖ Saved daily summary for ${user.email} (Status: ${status})`);
        }
    }
}

// 0. SAFETY CONFIG: Reddit requires a unique User-Agent and ~60 req/min limit
const parser = new Parser({
    customFields: {
        item: [['content:encoded', 'contentEncoded']],
    }
});

// Robust Fetch Utility to avoid 403 Forbidden
async function fetchRedditRSS(subreddit: string) {
    const url = `https://www.reddit.com/r/${subreddit}/new/.rss`;
    const userAgents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 RedLeads/1.1',
        'RedLeadsBot/1.1 (Lead Intelligence Tool; contact redleads.app@gmail.com)',
        'social-monitor:redleads:v1.1 (by /u/RedLeadsAdmin)'
    ];
    
    const response = await fetch(url, {
        headers: {
            'User-Agent': userAgents[Math.floor(Math.random() * userAgents.length)],
            'Accept': 'application/rss+xml, application/xml, text/xml',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
    });

    if (response.status === 403) {
        throw new Error('403 Forbidden: Reddit blocked our User-Agent or IP.');
    }

    if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    const xml = await response.text();
    return await parser.parseString(xml);
}

/**
 * THE RSS SENTINEL
 * Runs in a loop, staggered fetching subreddits.
 */
// --- Graceful Shutdown Logic ---
let isRunning = true;
const shutdown = () => {
    console.log('üõë Receive shutdown signal. Stopping Sentinel gracefully...');
    isRunning = false;
};
process.on('SIGTERM', shutdown);
process.on('SIGINT', shutdown);

async function runSentinel() {
    console.log(`üöÄ Sentinel Started: ${new Date().toISOString()}`);

    while (isRunning) {
        try {
            await sendHeartbeat();

            // 1. CLEANUP: Delete unsaved leads older than 14 days
            // This keeps the DB lean and fast.
            const fourteenDaysAgo = new Date();
            fourteenDaysAgo.setDate(fourteenDaysAgo.getDate() - 14);
            
            const { error: cleanupError } = await supabase
                .from('monitored_leads')
                .delete()
                .eq('is_saved', false)
                .lt('created_at', fourteenDaysAgo.toISOString());

            if (cleanupError) console.error('‚ö†Ô∏è Cleanup failed:', cleanupError.message);
            else console.log('üßπ Maintenance: Purged unsaved leads older than 14 days.');

            // 2. Fetch all unique subreddits across all users
            const { data: profiles, error } = await supabase
                .from('profiles')
                .select('id, subreddits, description, keywords')
                .not('subreddits', 'is', null)
                .neq('subreddits', '{}'); // Filter out empty arrays

            if (error) throw error;

            // 2. Build Monitor Map (Subreddit -> Users tracking it)
            const monitorMap: Record<string, any[]> = {};
            profiles.forEach(p => {
                p.subreddits?.forEach((sub: string) => {
                    const cleanSub = sub.replace('r/', '').trim();
                    if (!monitorMap[cleanSub]) monitorMap[cleanSub] = [];
                    monitorMap[cleanSub].push(p);
                });
            });

            const subreddits = Object.keys(monitorMap);
            console.log(`üìä Tracking ${subreddits.length} subreddits for ${profiles.length} users.`);

            // 3. Dynamic Batching (Safe Mode)
            // Rule: NEVER exceed 60 requests/minute to prevent IP ban.
            // Safe Target: 40 requests/minute.
            
            const PAUSE_SECONDS = 30;
            const TARGET_CYCLE_MINUTES = 30; // 30m is the sweet spot (Fast enough to win, slow enough to be safe)
            const MAX_REQ_PER_MINUTE = 40; 
            const MAX_BATCH_SIZE = Math.floor((MAX_REQ_PER_MINUTE * PAUSE_SECONDS) / 60); // = 20
            
            // We override the previous 10-minute goal if it requires unsafe speeds.
            // calculate safe batch size
            const calculatedBatch = Math.ceil(subreddits.length / (TARGET_CYCLE_MINUTES * 60 / PAUSE_SECONDS)); // target 15 min (900s)
            
            const BATCH_SIZE = Math.min(MAX_BATCH_SIZE, Math.max(5, calculatedBatch));

            console.log(`üõ°Ô∏è Rate Limit Guard: Capped at ${BATCH_SIZE} reqs per ${PAUSE_SECONDS}s (~${(BATCH_SIZE * 60)/PAUSE_SECONDS} req/min).`);

            for (let i = 0; i < subreddits.length; i += BATCH_SIZE) {
                const batch = subreddits.slice(i, i + BATCH_SIZE);
                console.log(`üì° Fetching Batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(subreddits.length/BATCH_SIZE)}: ${batch.length} subs`);

                await Promise.all(batch.map(async (sub, idx) => {
                    if (!isRunning) return; // Stop if shutdown signaled
                    
                    // Stagger individual requests within the batch to mimic human browsing
                    await new Promise(r => setTimeout(r, idx * 2000));
                    
                    try {
                        const feed = await fetchRedditRSS(sub);
                        
                        for (const item of feed.items as any[]) {
                            const postID = item.id || item.link;
                            const title = item.title || '';
                            const content = item.contentSnippet || '';
                            const url = item.link || '';

                            // Process this post for each user tracking this subreddit
                            for (const user of monitorMap[sub]) {
                                // Check if already processed for this user
                                const { data: existing } = await supabase
                                    .from('monitored_leads')
                                    .select('id')
                                    .eq('user_id', user.id)
                                    .eq('external_id', postID)
                                    .single();

                                if (existing) continue;

                                // LAYER 1: Priority Keywords Check
                                // If the user has defined keywords, the post MUST contain at least one.
                                const combinedText = `${title} ${content}`.toLowerCase();
                                
                                if (user.keywords && user.keywords.length > 0) {
                                    const hasKeyword = user.keywords.some((k: string) => combinedText.includes(k.toLowerCase()));
                                    if (!hasKeyword) continue; // ‚ùå Layer 1 Fail: No keyword match
                                } else {
                                    // Fallback: If no keywords set, check for generic intent signals
                                    const leadSignals = ['how to', 'problem', 'recommend', 'alternative', 'best', 'help', 'looking for'];
                                    const hasSignal = leadSignals.some(s => combinedText.includes(s));
                                    if (!hasSignal) continue;
                                }

                                console.log(`üéØ Layer 1 Passed for ${user.id} in r/${sub}: ${title}`);

                                // LAYER 2: AI Confirmation (Product Description Context)
                                const isMatch = await checkLeadRelevance(title, content, user.description, user.keywords);

                                if (isMatch) {
                                    await supabase.from('monitored_leads').insert({
                                        user_id: user.id,
                                        external_id: postID,
                                        title: title,
                                        subreddit: sub,
                                        url: url,
                                        match_score: 0.9,
                                        status: 'new'
                                    });
                                    console.log(`‚úÖ Lead Verified & Saved for ${user.id}`);
                                }
                            }
                        }
                    } catch (e) {
                        console.error(`‚ùå Error fetching r/${sub}:`, e);
                    }
                }));

                // Wait between batches
                if (isRunning && (i + BATCH_SIZE < subreddits.length)) {
                    console.log(`‚è≥ Pace keeper: ${PAUSE_SECONDS}s pause...`);
                    await new Promise(r => setTimeout(r, PAUSE_SECONDS * 1000)); 
                }
            }

        } catch (err) {
            console.error('Sentinel Loop Error:', err);
        }

        // --- SCHEDULER: Check if it's 00:00 UTC (approx) ---
        // We check if current hour is 0 and we haven't run it yet today? 
        // Simpler: Just run it every cycle if hour === 0 && lastRun != today
        // For simplicity implies we store state.
        // Quickest hack: If time is between 00:00 and 00:30 UTC and we assume the loop takes min 15m, it might run twice.
        // Better: Let's use a simple in-memory flag for 'lastReportDate'
        
        const now = new Date();
        const currentDateStr = now.toISOString().split('T')[0];
        
        // Ref: lastReportDate needs to be declared outside loop
        if (now.getUTCHours() === 0 && lastReportDate !== currentDateStr) {
             await generateDailyReport();
             lastReportDate = currentDateStr;
        }

        console.log('üîÑ Cycle complete. Restarting in 5 minutes...');
        await new Promise(r => setTimeout(r, 60000)); // Sleep just 1 min for testing logic, or revert to 5? 
        // User asked for 15m cycle earlier, wait logic logic was dynamic.
        // Reverting to dynamic wait logic from before (variable PAUSE was used for batch, this is CYCLE pause)
        // Actually the code structure I see in 'read_url' was a bit different than I recall?
        // Let's stick to the code structure I see in the file.
        // The previous code had "await new Promise(r => setTimeout(r, 300000)); // 5 minute total rest".
        // I will keep the existing 5 min rest but add the scheduling check.
        await new Promise(r => setTimeout(r, 300000)); 
    }
}
// State for scheduler
let lastReportDate = '';

async function checkLeadRelevance(title: string, content: string, description: string, keywords: string[]): Promise<boolean> {
    try {
        const prompt = `
            BUSINESS CONTEXT: ${description}
            TARGET KEYWORDS: ${keywords?.join(', ')}
            
            REDDIT POST:
            Title: ${title}
            Content: ${content}
            
            Does this post represent a potential customer or someone needing the solution described above? 
            Respond with ONLY "YES" or "NO".
        `;

        const data = await groq.call({
            model: "llama-3.1-8b-instant", // ‚ö° High speed, ultra low cost for high-volume scanning
            messages: [{ role: "user", content: prompt }],
            temperature: 0,
        });

        const result = data.choices?.[0]?.message?.content?.trim().toUpperCase();
        return result === 'YES';
    } catch (e) {
        return false;
    }
}

/**
 * AI Summary Helper using the High-Quality 70B Model
 */
async function generateAISummary(leads: any[], userContext: string): Promise<string> {
    if (leads.length === 0) return "";

    try {
        const prompt = `
            USER BUSINESS CONTEXT: ${userContext}
            
            UNPROCESSED LEADS FROM LAST 24H:
            ${leads.map(l => `- [r/${l.subreddit}] ${l.title}`).join('\n')}
            
            Write a 2-3 sentence high-level executive summary for this user about their best opportunities today. 
            Focus on intent and potential value. Keep it professional and concise.
        `;

        const data = await groq.call({
            model: "llama-3.3-70b-versatile", // üß† High quality for the daily summary
            messages: [{ role: "user", content: prompt }],
            temperature: 0.5,
        });

        return data.choices?.[0]?.message?.content?.trim() || "";
    } catch (e) {
        return "";
    }
}

runSentinel();
